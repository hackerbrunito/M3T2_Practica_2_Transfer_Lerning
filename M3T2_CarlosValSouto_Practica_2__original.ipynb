{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7cfb882",
      "metadata": {
        "id": "e7cfb882"
      },
      "source": [
        "# Práctica 2: Reutilización de grandes modelos neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b801bd8e",
      "metadata": {
        "id": "b801bd8e"
      },
      "source": [
        "En la segunda práctica del módulo vamos a utilizar la técnica más extendida para el uso productivo de redes neuronales: la **reutilización**, aka, **transfer learning**. Usaremos esta tecnica para dar solución al problema el cual hemos enfrentado en la práctica 1 sin mucho éxito. El dataset que vamos a utilizar es el mismo que en la Practica 1, que se puede encontrar [aquí](https://www.kaggle.com/datasets/anshtanwar/pets-facial-expression-dataset).\n",
        "\n",
        "La parte final de esta práctica tiene como objetivo enfrentar de principio a fin un caso de uso utilizando Redes Neuronales Convolucionales reutilizadas.\n",
        "\n",
        "La practica tiene **dos ejercicios**:\n",
        "\n",
        "1. Ej 1. Aprender a reutilizar modelos (5 puntos)\n",
        "2. Ej 2. Problema de clasificación end-to-end (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dz19wU6dZxRE",
      "metadata": {
        "id": "Dz19wU6dZxRE"
      },
      "source": [
        "### Objetivos\n",
        "\n",
        "Los objetivos esta primera práctica son los siguientes:\n",
        "- Reutilización de modelos y su adaptación a problemas concretos.\n",
        "- Utilización de técnicas de *data augmentation* utilizando PyTorch.\n",
        "- Utilización de modelos y pesos pre-entrenados de PyTorch.\n",
        "- Enfrentar un problema de reutilización desde el principio hasta el final.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jteHjSe-Zyz6",
      "metadata": {
        "id": "jteHjSe-Zyz6"
      },
      "source": [
        "### Consideraciones generales en el uso de Notebooks\n",
        "- Asegúrate de que todo el código necesario para ejecutarlo completamente está disponible.\n",
        "- Asegúrate de usar un orden lógico de ejecución de celdas para permitir una ejecución completa y automática. El Notebook debe poder funcionar perfectamente simplemente lanzando la opción \"Restart & Run All\" de la sección \"Kernel\". Cuidado con instanciar variables o métodos en celdas posteriores a las cuales están utilizadas, una ejecución independiente y manual puede hacer que funcione, pero la ejecución automática fallará.\n",
        "- Asegúrate de utilizar las celdas de tipo \"Markdown\" para texto y las de \"Code\" para código. Combinar distintos tipos de celda ayuda a la explicabilidad de la resolución y a la limpieza en general.\n",
        "- Puedes crear celdas adicionales si lo necesitas.\n",
        "- A la hora de la entrega del Notebook definitivo, asegúrate que lo entregas ejecutado y con las salidas de las celdas guardadas. Esto será necesario para la generación del PDF de entrega.\n",
        "- El código cuanto más comentado mejor. Además, cualquier reflexión adicional siempre será bienvenida."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102a4ceb",
      "metadata": {
        "id": "102a4ceb"
      },
      "source": [
        "## Ej 1. Aprender a reutilizar modelos (5 puntos)\n",
        "\n",
        "El objetivo de la práctica de esta semana será utilizar otra estrategia enfrentar el problema que venimos arrastrando de la primera actividad: la clasificación de emociones en imágenes de animales.\n",
        "\n",
        "Como pudiste observar, diseñar una red neuronal convencional y adaptar los datos para su uso resultó en un rendimiento muy pobre: no logró aprender y sus predicciones se daban practicamente al  azar. Posteriormente, utilizamos una arquitectura convencional, concretamente una AlexNet, para intentar mejorar el rendimiento. El resultado, de nuevo, dejó mucho que desear.\n",
        "\n",
        "No sé si te has parado a analizar el problema que estamos enfrentando. ¿Has visto las fotos de los animales? ¿Sabrías decir si los pájaros están tristes, contentos...? En definitiva, es un problema complicado de resolver. Además, como recordarás de la teoría, las redes neuronales exigen muchos datos para ser entrenadas, más aún si hay que enfrentar un problema complejo como este. ¿Cuál es la realidad?, que únicamente contamos con 1000 imágenes para entrenar... ya hemos visto que no son suficientes para entrenar redes neuronales.\n",
        "\n",
        "Llegados a este punto, existen dos opciones que pueden ser perfectamente complementarias:\n",
        "1.  **Obtener más datos**. El problema es que no siempre es sencillo (ni barato). Para lograr obtener más datos la solución alternativa pasa por utilizar técnicas de *data augmentation*. Por ejemplo, se pueden utilizar modelos generativos (desde GANs hasta modelos de difusion como DALLE-2), o utilizar modelos de Style Transfer para generar variantes estilísticas de una misma imagen. Una alternativa muy comun y facil de implementar para *data augmentation* consiste en realizar **transformaciones a las imagenes**. Con más datos, podremos nutrir nuestro conjunto de entrenamiento y, casi con total seguridad, mejoraremos el rendimiento de nuestras redes. El problema es, ¿cuántos datos son suficientes? La respuesta: muchos. Y más aún si queremos entrenar una red desde 0.\n",
        "\n",
        "2.  **Utilizar Transfer Learning**. Podemos reutilizar tanto la arquitectura como los pesos de un modelo entrenado con buenas métricas para adaptarlo a nuestro problema en concreto y, con eso, solventar parcialmente la falta de datos.\n",
        "\n",
        "En esta practica vamos a desarrollar **la segunda opción**, ya que suele ser la más viable en problemas \"reales\". Ademas, si queremos aplicar data augmentation usando transformaciones, necesitaremos entrenar el modelo mas epochs. Esto es porque PyTorch cambia el contenido de cada imagen en cada época, no aumenta el\n",
        "tamaño del dataset directamente. Dejo como **ejercicio voluntario** el intentar aumentar el conjunto de datos para mejorar aún más el rendimiento de alguno de los modelos, teniendo en cuenta que habra que entrenar durante mas epochs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owTz2_yBfoxA",
      "metadata": {
        "id": "owTz2_yBfoxA"
      },
      "source": [
        "### Importar las librerias necesarias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3dcf7d2b",
      "metadata": {
        "id": "3dcf7d2b"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "06d7be32",
      "metadata": {
        "id": "06d7be32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H2Z44bB4ksDp",
      "metadata": {
        "id": "H2Z44bB4ksDp"
      },
      "source": [
        "### Ej 1.1: Carga de datos y modelo (0.5 puntos)\n",
        "\n",
        "Los datos que vamos a utilizar serán los mismos que en la Práctica 1, que los tienes disponibles [aqui](https://www.kaggle.com/datasets/anshtanwar/pets-facial-expression-dataset). Como opción alternativa a la descarga en local, se pueden tambien cargar en cache como se ofreció en la Practica 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Yy0z0Ecxktfw",
      "metadata": {
        "id": "Yy0z0Ecxktfw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"anshtanwar/pets-facial-expression-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9630e88",
      "metadata": {
        "id": "a9630e88"
      },
      "source": [
        "\n",
        "La diferencia reside en que en la Práctica 1, las transformaciones aplicadas sobre las imágenes en bruto (variable *transformations*) era diseñada por nosotros. En este caso, dado que vamos a utilizar un modelo pre-entrenado, habrá que adaptarse al formato de dato que espera el modelo para unos pesos en concreto. Dependiendo del diseño de la fuente de donde se carguen los modelos, las transformaciones se pueden obtener de distinta forma.\n",
        "\n",
        "En este caso, vamos a hacer uso de los modelos que Torchvision ofrece de forma nativa, que tenéis disponibles [aqui](https://pytorch.org/vision/stable/models.html). Concretamente, vamos a utilizar:\n",
        "\n",
        "- Un modelo **Efficient Net B1**.\n",
        "- Como pesos, la segunda versión (v2) de los entrenados sobre IMAGENET1K.\n",
        "\n",
        "En el caso de Torchvision, las transformaciones se pueden localizar en el atributo *transforms* de los pesos. Para recuperar el pipeline completo de pre-procesamiento, se puede hacer uso de `weights.transforms()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd2c3622",
      "metadata": {
        "id": "fd2c3622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado: EfficientNet B1 - v2\n",
            "Pesos utilizados: EfficientNet_B1_Weights.IMAGENET1K_V2\n",
            "Transformaciones obtenidas automáticamente:\n",
            "\n",
            " ImageClassification(\n",
            "    crop_size=[240]\n",
            "    resize_size=[255]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importar los pesos específicos de EfficientNet B1 v2\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "weights = EfficientNet_B1_Weights.IMAGENET1K_V2 # Completar\n",
        "model = efficientnet_b1(weights=weights) # Completar\n",
        "transformation = weights.transforms()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Modelo cargado: EfficientNet B1 - v2\")\n",
        "print(f\"Pesos utilizados: {weights}\")\n",
        "print(f\"Transformaciones obtenidas automáticamente:\\n\\n {transformation}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "10bd1f2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Angry/\n",
            "=> happy/\n",
            "=> Master Folder/\n",
            "     => test/\n",
            "     => train/\n",
            "     => valid/\n",
            "=> Other/\n",
            "=> Sad/\n"
          ]
        }
      ],
      "source": [
        "# No incluido en la práctic 2\n",
        "# Listado de los directorios del dtaset\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_base = r\"C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\"\n",
        "\n",
        "for item in os.listdir(dataset_base):\n",
        "   item_path = os.path.join(dataset_base, item)\n",
        "   if os.path.isdir(item_path):\n",
        "       print(f\"=> {item}/\")\n",
        "       # Ver subdirectorios\n",
        "       for sub_item in os.listdir(item_path):\n",
        "           sub_path = os.path.join(item_path, sub_item)\n",
        "           if os.path.isdir(sub_path):\n",
        "               print(f\"     => {sub_item}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4419e751",
      "metadata": {
        "id": "4419e751"
      },
      "source": [
        "Ya podemos entonces crear las instancias de **Dataset** para los tres conjuntos: `train_dataset`, `validation_dataset` y `test_dataset`. ¡No olvides aplicar las transformaciones recuperadas anteriormente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f8e3dd06",
      "metadata": {
        "id": "f8e3dd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset de entrenamiento: 1000 imágenes.\n",
            "Dataset de validación: 36 imágenes.\n",
            "Dataset de test: 38 imágenes.\n",
            "El dataset contiene '4' clases  > >> ['Angry', 'Other', 'Sad', 'happy'] \n"
          ]
        }
      ],
      "source": [
        "# Path de kagglehub para la ubicación de los datos.\n",
        "data_path = r\"C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\\Master Folder\"\n",
        "\n",
        "# Crear los datasets con las transformaciones del modelo pre-entrenado\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'train'),\n",
        "    transform=transformation\n",
        ")\n",
        "validation_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'valid'),\n",
        "    transform=transformation\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'test'),\n",
        "    transform=transformation\n",
        ")\n",
        "\n",
        "print(f\"Dataset de entrenamiento: {len(train_dataset)} imágenes.\")\n",
        "print(f\"Dataset de validación: {len(validation_dataset)} imágenes.\")\n",
        "print(f\"Dataset de test: {len(test_dataset)} imágenes.\")\n",
        "print(f\"El dataset contiene '{len(train_dataset.classes)}' clases  > >> {train_dataset.classes} \")\n",
        "# print(f\"Numero de clases: {len(train_dataset.classes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abec152",
      "metadata": {
        "id": "3abec152"
      },
      "source": [
        "Creamos los **DataLoader** correspondientes a cada **Dataset**: `train_dataloader`, `validation_dataloader` y `test_dataloader`. Utilizaremos para todos ellos `batch_size` de 64, con el objetivo de agilizar el entrenamiento, y mezcla en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e6fef3b",
      "metadata": {
        "id": "2e6fef3b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b71a966",
      "metadata": {
        "id": "3b71a966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoaders creados:\n",
            "- Train: 16 batches de 64\n",
            "- Validation: 1 batches de 64\n",
            "- Test: 1 batches de 64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configurar parámetros de carga\n",
        "batch_size = 64  # Tamaño de batch \n",
        "num_workers = 4  # Núcleos de CPU para cargar datos en paralelo\n",
        "\n",
        "# Crear los DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,        # Mezclar datos de entrenamiento\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True      # Optimización para GPU\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,       # No mezclar validación (orden no importa)\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,       # No mezclar test\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders creados:\")\n",
        "print(f\"- Train: {len(train_dataloader)} batches de {batch_size}\")\n",
        "print(f\"- Validation: {len(validation_dataloader)} batches de {batch_size}\")\n",
        "print(f\"- Test: {len(test_dataloader)} batches de {batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1280bba",
      "metadata": {
        "id": "e1280bba"
      },
      "source": [
        "### Ej 1.2: Adaptación del modelo (2 puntos)\n",
        "\n",
        "Los pesos que hemos seleccionado han sido entrenados utilizando IMAGENET en su versión con 1000 etiquetas. Nuestro problema es diferente, por lo que tendremos que adaptar el modelo.\n",
        "\n",
        "El objetivo será \"congelar\" toda la parte convolucional del modelo, es decir, el extractor de características. Congelar implica que los parámetros no se entrenen, por dos motivos:\n",
        "- No queremos \"ensuciar\" el entrenamiento que se ha realizado.\n",
        "- No tenemos recursos suficientes para entrenar una red con este número de parámetros.\n",
        "\n",
        "Posteriormente, modificaremos el clasificador para adaptar el modelo a nuestro problema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5c6a51",
      "metadata": {
        "id": "6c5c6a51"
      },
      "source": [
        "Muestra la arquitectura del modelo cargado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2ebab975",
      "metadata": {
        "id": "2ebab975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquitectura completa del modelo EfficientNet B1:\n",
            "EfficientNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2dNormActivation(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (2): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (2): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
            "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
            "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
            "      )\n",
            "      (3): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
            "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
            "      )\n",
            "      (3): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
            "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
            "      )\n",
            "      (3): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
            "      )\n",
            "      (4): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
            "            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (3): Conv2dNormActivation(\n",
            "            (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (8): Conv2dNormActivation(\n",
            "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=True)\n",
            "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "============================================================\n",
            "Número total de parámetros: 7,794,184\n"
          ]
        }
      ],
      "source": [
        "# Mostrar la arquitectura del modelo cargado\n",
        "print(\"Arquitectura completa del modelo EfficientNet B1:\")\n",
        "print(model)\n",
        "print(\"\\n\" + \"===\"*20)\n",
        "print(f\"Número total de parámetros: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e050b17",
      "metadata": {},
      "source": [
        "### Comentario de la casilla anterior\n",
        "\n",
        "En la casilla anterior se lista la arquitectura del algoritmo EfficientNet B1, el cual tiene tres secciones principales:\n",
        "1. (features)   =   es el extractor de características. Son todas las capas convolucionales que extraen patrones como bordes, formas, texturas, objetos...\n",
        "2. (avgpool)    =   el Pooling promedio. Ayduda a reducir el tamaño. Convierte los mapas de características a vector.\n",
        "3. (classifier) =   Se trata del clasificador y es la parte final del proceso que usa una red clasica o Red Densa (Fully Connected Network). Contiene 2 capas: Dropout + Linear. REduce las clases a las 4 clases finales y toma la decisión de a que clase pertenecen los datos que le van llegando (las 4 emociones)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567ac166",
      "metadata": {
        "id": "567ac166"
      },
      "source": [
        "Congela los parámetros de toda la red, es decir, configura los parámetros para que no requieran el cómputo del gradiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "28b5d4c0",
      "metadata": {
        "id": "28b5d4c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parámetros totales: 7,794,184\n",
            "Parámetros congelados: 6,513,184\n",
            "Parámetros entrenables: 1,281,000\n"
          ]
        }
      ],
      "source": [
        "# Congelar todos los parámetros del extractor de características. Itera sobre todos los parámetros de la sección \"features\" con model.features.parameters()\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Verificar cuántos parámetros están congelados vs entrenables\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(f\"Parámetros totales: {total_params:,}\")\n",
        "print(f\"Parámetros congelados: {frozen_params:,}\")\n",
        "print(f\"Parámetros entrenables: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b605b988",
      "metadata": {},
      "source": [
        "### Comentario de la casilla anterior\n",
        "\n",
        "El bucle for itera por cada uno de los parámetros de la sección features del model y los congela con \"param.requires_grad = False\".\n",
        "\n",
        "El siguiente bloque verifica cuantos elementos hay en el modelo con la funciond de Pytorch .numel() y los divide en totales y los que estan congelados. Por último, la resta de estos dos valores nos da el total de parametros entrenables, que en realidad es el paso donde el clasifier reduce la clasificación a 1000 clases, que hace lo siguiente: 1000 + bias nos da = 1.281.000 parametros entrenables (si lo he entendido bien)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954bf69e",
      "metadata": {
        "id": "954bf69e"
      },
      "source": [
        "Localiza la capas o capas que actúan como clasificadores. ¿Cuántas son? ¿Cómo se llama el bloque que las contiene?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "zxC__u5E0mon",
      "metadata": {
        "id": "zxC__u5E0mon"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " BLOQUES PRINCIPALES \n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "  1. features\n",
            "  2. avgpool\n",
            "  3. classifier\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            " Capas dentro del bloque 'classifier':\n",
            "  1. Dropout\n",
            "  2. Linear\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            " El bloque clasificador se llama: classifier\n",
            " Contiene 2 capas: Dropout, Linear\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n"
          ]
        }
      ],
      "source": [
        "# mostrar los bloques principales del modelo\n",
        "print(\" BLOQUES PRINCIPALES \")\n",
        "print(\"¨\" * 60)\n",
        "bloques = list(model.named_children())\n",
        "for i, (name, _) in enumerate(bloques):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "print(\"¨\" * 60)\n",
        "# Obtener  los nombres de las capas dentro del clasificador\n",
        "print(f\" Capas dentro del bloque 'classifier':\")\n",
        "capas_clasificador = [type(capa).__name__ for capa in model.classifier]\n",
        "for i, capa in enumerate(capas_clasificador):\n",
        "    print(f\"  {i+1}. {capa}\")\n",
        "    \n",
        "# print(f\"Capas dentro del bloque 'classifier': {capas_clasificador}\")\n",
        "\n",
        "print(\"¨\" * 60)\n",
        "print(f\" El bloque clasificador se llama: {bloques[2][0]}\")\n",
        "print(f\" Contiene {len(model.classifier)} capas: {', '.join(capas_clasificador)}\")\n",
        "print(\"¨\" * 60)\n",
        "\n",
        "\n",
        "# print(f\"• Contiene {len(model.classifier)} capas: {', '.join(capas_clasificador)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5a1241",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "\n",
        "### En el código anterior hay 3 partes:\n",
        "\n",
        "La primera lista los bloques principales que contiene el modelo. \n",
        "\n",
        "La segunda muestra las capas dentro del modelo clasificador. \n",
        "\n",
        "La tercera es simplemente una respuesta resumida de lo anterior.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ce4722",
      "metadata": {
        "id": "64ce4722"
      },
      "source": [
        "Define un nuevo clasificador adaptado a nuestro problema:\n",
        "*   Una capa de Dropout con probabilidad de **0.3** y que realice la operación **inplace**.\n",
        "*   Una capa Linear adecuada: hay que definir el número de *features* de entrada y el número de *features* de salida.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7085fe0",
      "metadata": {
        "id": "f7085fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modelo completo transferido a: cuda\n",
            "✅ Clasificador en: cuda:0\n"
          ]
        }
      ],
      "source": [
        "in_features = model.classifier[1].in_features\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "new_head = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Transferir el nuevo clasificador a CUDA ANTES de asignarlo\n",
        "new_head = new_head.to(device)\n",
        "model.classifier = new_head\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\"✅ Modelo completo transferido a: {device}\")\n",
        "print(f\"✅ Clasificador en: {next(model.classifier.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea0400ed",
      "metadata": {
        "id": "ea0400ed"
      },
      "source": [
        "Actualiza el clasificador del modelo. Por defecto, el nuevo clasificador no tendrá los pesos congelados y, por tanto, se entrenará."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e32792ea",
      "metadata": {
        "id": "e32792ea"
      },
      "outputs": [],
      "source": [
        "# Actualizar el clasificador del modelo\n",
        "model.classifier = new_head\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d653108c",
      "metadata": {
        "id": "d653108c"
      },
      "source": [
        "Muestra y verifica que el modelo está actualizado con la nueva arquitectura:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4ecd538a",
      "metadata": {
        "id": "4ecd538a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "   CLASIFICADOR ACTUALIZADO \n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Nuevo clasificador:\n",
            "Sequential(\n",
            "  (0): Dropout(p=0.3, inplace=True)\n",
            "  (1): Linear(in_features=1280, out_features=4, bias=True)\n",
            ")\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n",
            "Dimensiones de la capa Linear:\n",
            "Entrada: 1280\n",
            "Salida: 4\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n"
          ]
        }
      ],
      "source": [
        "# Verificar que se ha actualizado correctamente\n",
        "print(\"^\" * 60)\n",
        "print(\"   CLASIFICADOR ACTUALIZADO \")\n",
        "print(\"^\" * 60)\n",
        "\n",
        "print(f\"Nuevo clasificador:\")\n",
        "print(model.classifier)\n",
        "print(\"¨\" * 60)\n",
        "\n",
        "print(f\"\\nDimensiones de la capa Linear:\")\n",
        "print(f\"Entrada: {model.classifier[1].in_features}\")\n",
        "print(f\"Salida: {model.classifier[1].out_features}\")\n",
        "print(\"¨\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12fef29",
      "metadata": {},
      "source": [
        "### Comentario del código anterior\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "\n",
        "Con anterioridad a este paso, obtuvimos que:\n",
        "    \n",
        "(0): Dropout(p=0.2, inplace=True)\n",
        "\n",
        "(1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "\n",
        "\n",
        "\n",
        "</td>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "Después de actualizar de clasificador tenemos:\n",
        "  \n",
        "(0): Dropout(p=0.3, inplace=True)\n",
        "  \n",
        "(1): Linear(in_features=1280, out_features=4, bias=True)\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af82f39",
      "metadata": {
        "id": "2af82f39"
      },
      "source": [
        "### Ej 1.3: Checkpoints (1.5 puntos)\n",
        "\n",
        "Ya tenemos todo listo para poder entrenar nuestro modelo. El único problema es que el entrenamiento va a ser costoso (aunque podría serlo muchísimo más), por lo que lo correcto es utilizar *checkpoints* para evitar cualquier inconveniente que pueda ocurrir durante el entrenamiento que haga que el entrenamiento se detenga y, por tanto, echar a perder todo el cómputo realizado.\n",
        "\n",
        "Es por ello que antes que nada vamos a crear dos funciones que permitan guardar y cargar un *checkpoint* utilizando las herramientas que nos proporciona PyTorch. Recuerda guardar todo lo que sea necesario para que el modelo pueda ser recuperado íntegramente o, al menos, que pueda seguir entrenando por donde iba.\n",
        "\n",
        "En PyTorch, el formato para guardar *checkpoints* es *.pth*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974e9cdc",
      "metadata": {
        "id": "974e9cdc"
      },
      "source": [
        "#### Método para guardar un *checkpoint*\n",
        "- **path**: Dirección donde se va a almacenar el *checkpoint*\n",
        "- **model**: Instancia del modelo a almacenar\n",
        "- **optimizer**: Optimizador utilizado, con el objetivo de guardar su estado (recuerda que hay optimizadores con hiperparámetros progresivos).\n",
        "\n",
        "El método no devuelve nada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c38c901e",
      "metadata": {
        "id": "c38c901e"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, model, optimizer):\n",
        "    \n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint guardado en: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02005825",
      "metadata": {
        "id": "02005825"
      },
      "source": [
        "#### Método para recuperar un *checkpoint*\n",
        "- **path**: Dirección donde se va a recuperar el *checkpoint*\n",
        "- **model**: Instancia \"vacía\" del modelo recuperado. La arquitectura debe ser la misma que a la hora del guardado.\n",
        "- **optimizer**: Instancia nueva del optimizador recuperado. Debe ser del mismo tipo para poder actualizar su estado.\n",
        "\n",
        "El método debe devolver:\n",
        "- **model**: Modelo actualizado.\n",
        "- **optimizer**: Optimizador actualizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "068912e5",
      "metadata": {
        "id": "068912e5"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    print(f\"Checkpoint cargado desde: {path}\")\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89b5d71",
      "metadata": {
        "id": "e89b5d71"
      },
      "source": [
        "### Ej 1.4: Entrenamiento del modelo (1.0 punto)\n",
        "\n",
        "Finalmente, ya tenemos toda la información y herramientas necesarias para proceder a entrenar el modelo. A continuacion, se ofrecen los metodos para entrenar y evaluar los conjuntos de validation/test que ya se trabajaron en la Practica 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aeff4d51",
      "metadata": {
        "id": "aeff4d51"
      },
      "outputs": [],
      "source": [
        "def training(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    model.train() # Indicamos al modelo que vamos a entrenar\n",
        "\n",
        "    num_batches = len(dataloader) # Número de batches para promediar el loss\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader): # Iteramos por los distintos batches creados en el dataloader\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X) # Calculamos el forward pass\n",
        "        loss = loss_fn(pred, y) # Calculamos el loss\n",
        "\n",
        "        _,predicted=torch.max(pred,1)\n",
        "        correct+=(y==predicted).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # Limpiamos los gradientes viejos\n",
        "        loss.backward() # Calculamos los nuevos gradientes\n",
        "        optimizer.step() # Actualizamos los parámetros (pesos) del modelo\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    correct /= size\n",
        "    avg_train_loss = train_loss/num_batches\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Train Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg train loss: {avg_train_loss:>8f}\\n\")\n",
        "    return avg_train_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6c5cc412",
      "metadata": {
        "id": "6c5cc412"
      },
      "outputs": [],
      "source": [
        "def test_and_validation(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() # Indicamos que no vamos a entrenar\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            _,predicted=torch.max(pred,1)\n",
        "            correct+=(y==predicted).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    avg_loss = test_loss\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Val or Test Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934dd291",
      "metadata": {
        "id": "934dd291"
      },
      "source": [
        "Utilizaremos **Adam** como optimizador y **CrossEntropyLoss** como función de coste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "199f43fd",
      "metadata": {
        "id": "199f43fd"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c4e96e",
      "metadata": {
        "id": "03c4e96e"
      },
      "source": [
        "Entrena durante **4 épocas** el modelo guardando al final de cada una de ellas un *checkpoint*. Puedes reescribir siempre el mismo o crear un *checkpoint* para cada una de las épocas.\n",
        "\n",
        "**Atención**: Para simplemente probar el código te recomiendo que pruebes únicamente con una época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b175b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponible: True\n",
            "Device actual: cuda\n",
            "Modelo en device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# verificando que el \"device\" es --->> cuda\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "print(f\"Device actual: {device}\")\n",
        "print(f\"Modelo en device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf16d1c",
      "metadata": {
        "id": "daf16d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 79.5%, Avg train loss: 0.674906\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 52.8%, Avg loss: 1.255031 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_1.pth\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 2\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 80.6%, Avg train loss: 0.663411\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 52.8%, Avg loss: 1.258762 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_2.pth\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 3\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 81.7%, Avg train loss: 0.665443\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.268455 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_3.pth\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 4\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 83.6%, Avg train loss: 0.632432\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.273572 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_4.pth\n"
          ]
        }
      ],
      "source": [
        "epochs = 4\n",
        "for t in range(epochs):\n",
        "    print(f\"\\n-------------------------------\\n  ==> Epoch - {t+1}\\n-------------------------------\\n\")\n",
        "    \n",
        "    # Entrenar\n",
        "    training(train_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    # Validar\n",
        "    test_and_validation(validation_dataloader, model, loss_fn)\n",
        "    \n",
        "    # Guardar checkpoint al final de cada época\n",
        "    save_checkpoint(f\"checkpoint_epoch_{t+1}.pth\", model, optimizer)\n",
        "    \n",
        "    print(f\"\\n-------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e476bde",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "Tras repetidas ejecuciones, 5 ciclos consecutivos del entrenamiento han dado estos outputs:\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "**Train Accuracy**  \n",
        "  \n",
        "**Ejecución 1**: 36.5% → 72.1%  \n",
        "**Ejecución 2**: 72.8% → 76.3%   \n",
        "**Ejecución 3**: 76.3% → 77.8%  \n",
        "**Ejecución 4**: 80.2% → 78.5%  \n",
        "**Ejecución 5**: 79.5% → 83.6%  \n",
        "\n",
        "\n",
        "</td>\n",
        "<td>\n",
        "\n",
        "**Validation Accuracy**  \n",
        "  \n",
        "**Ejecución 1**: 41.7% → 52.8%  \n",
        "**Ejecución 2**: 52.8% → 52.8%  \n",
        "**Ejecución 3**: 52.8% → 52.8%  \n",
        "**Ejecución 4**: 52.8% → 50.0%  \n",
        "**Ejecución 5**: 52.8% → 50.0%  \n",
        "\n",
        "</td>\n",
        "<td>\n",
        "\n",
        "**Validation Loss**  \n",
        "  \n",
        "**Ejecución 1**: 1.309 → 1.243  \n",
        "**Ejecución 2**: 1.227 → 1.226  \n",
        "**Ejecución 3**: 1.227 → 1.236  \n",
        "**Ejecución 4**: 1.246 → 1.243  \n",
        "**Ejecución 5**: 1.255 → 1.274  \n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Observaciones:\n",
        "  \n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "- **Train Accuracy**: Obtiene una mejora progresiva de más del 70%\n",
        "- **Validation Accuracy**: Muestra un estancamiento sobre ~50-53%\n",
        "- **Validation Loss**: Se mantiene estable ~1.24-1.27\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "El estancamiento del `Validation` nos da a entender que hay `OverFitting`. Un resultado óptimo sería cuando: \n",
        "\n",
        "\n",
        "- El Accuracy de `Training` y `Validation` tuviera en ambos una subida progresiva y mantuvieran un \"gap\" del 10-15% o menos de diferencia.\n",
        "- El `Loss` de Train y Validation tuviera en ambos una bajada progresiva.\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e48d71",
      "metadata": {
        "id": "67e48d71"
      },
      "source": [
        "Ahora, vamos a utilizar los **checkpoints** para entrenar el modelo mas epochs.\n",
        "\n",
        "Carga el último **checkpoint** generado en la útima época del último entrenamiento en las nuevas variables `model_recovered` y `optimizer_recovered` para entrenar el modelo **4 épocas más**.\n",
        "\n",
        "Consideraciones:\n",
        "- El loss en test de la última época del primer entrenamiento **debe coincidir** con el loss en test antes de la primera época del segundo entrenamiento. Muestra ambos valores para verificar que, efectivamente, estás entrenando el \"mismo\" modelo.\n",
        "- Para cargar un *checkpoint* la arquitectura del modelo debe ser la misma que a la hora de guardarlo. Si mantienes la misma sesion de colab y tienes el modelo **Efficient Net B1** correctamnte adaptado y en la variable `model`, puedes usar esta variable para cargar el checkpoint. Si este no es el caso, deberás crear una instancia nueva del Efficient Net y repetir el proceso seguido en el ejercicio 1.2 para adaptar su arquitectura y, con ello, poder cargar los pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "f60e9245",
      "metadata": {
        "id": "f60e9245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Carlos\\AppData\\Local\\Temp\\ipykernel_3772\\3128740935.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint cargado desde: checkpoint_epoch_4.pth\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n",
            "Loss de test de la última época del primer entrenamiento: 1.273572\n",
            "\n",
            "Evaluando modelo cargado ANTES del segundo entrenamiento:\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.273572 \n",
            "\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n",
            "^ Este loss DEBE coincidir con 1.273572 para verificar continuidad\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
        "# Cargar el último checkpoint en nuevas variables\n",
        "print(\"¨\" * 60 + \"\\n\")\n",
        "model_recovered = model  # Reutilizar modelo existente (misma sesión)\n",
        "\n",
        "# Crear optimizador IGUAL al original (solo classifier.parameters)\n",
        "optimizer_recovered = torch.optim.Adam(model_recovered.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Cargar el último checkpoint\n",
        "checkpoint = load_checkpoint(\"checkpoint_epoch_4.pth\", model_recovered, optimizer_recovered)\n",
        "\n",
        "print(\"¨\" * 60 + \"\\n\")\n",
        "# Verificación -> Comparar loss de test\n",
        "print(f\"Loss de test de la última época del primer entrenamiento: 1.273572\")\n",
        "print()\n",
        "print(\"Evaluando modelo cargado ANTES del segundo entrenamiento:\")\n",
        "print()\n",
        "test_and_validation(validation_dataloader, model_recovered, loss_fn)\n",
        "print(\"¨\" * 60 + \"\\n\")\n",
        "print(\"^ Este loss DEBE coincidir con 1.273572 para verificar continuidad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72a006c",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "\n",
        "`Loss` = 1.273572  ->  ambos coinciden (época 4 del entrenamiento anterior y checkpoit cargado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455f4959",
      "metadata": {
        "id": "455f4959"
      },
      "source": [
        "Ahora podemos proceder con el re-entrenamiento con el modelo recien cargado usando el checkpoint. Implementa el codigo para entrenar **4 epochs mas** el modelo en la variable `model_recovered`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "444bbd43",
      "metadata": {
        "id": "444bbd43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 82.2%, Avg train loss: 0.640110\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.269344 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_1.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 2\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 83.0%, Avg train loss: 0.623642\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.277620 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_2.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 3\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 82.4%, Avg train loss: 0.617482\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.275251 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_3.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 4\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 83.4%, Avg train loss: 0.604949\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.271064 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_4.pth\n",
            "\n",
            "-------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 4\n",
        "for t in range(epochs):\n",
        "    print(f\"\\n-------------------------------\\n  ==> Epoch - {t+1}\\n-------------------------------\\n\")\n",
        "    \n",
        "        # Entrenar\n",
        "    training(train_dataloader, model_recovered, loss_fn, optimizer_recovered)\n",
        "    \n",
        "    # Validar\n",
        "    test_and_validation(validation_dataloader, model_recovered, loss_fn)\n",
        "    \n",
        "    # Guardar checkpoint con nomenclatura diferente\n",
        "    save_checkpoint(f\"checkpoint_recovered_epoch_{t+1}.pth\", model_recovered, optimizer_recovered)\n",
        "    \n",
        "    print(f\"\\n-------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwmEV1P3fiQw",
      "metadata": {
        "id": "FwmEV1P3fiQw"
      },
      "source": [
        "**Responde a las siguientes preguntas:**\n",
        "1. ¿Mejora este modelo una clasificación al azar?\n",
        "\n",
        "2. ¿Sufre el modelo de Overfitting o Underfitting?\n",
        "\n",
        "3. ¿Crees que podría seguir mejorando sus métricas? ¿Cómo?\n",
        "\n",
        "Puedes hacer uso (optativo) de la funcion `plot_comparative` de la Practica 1 para visualizar el `accuracy` con respecto a las epochs de entrenamiento. Para ello deberas guardar en una lista el `accuracy` durante el entrenamiento.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_PP1-BHyfnfA",
      "metadata": {
        "id": "_PP1-BHyfnfA"
      },
      "source": [
        "### Respuesta\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "\n",
        "<td width=\"100%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "1. No hay mejoría significativa, la mejora es mínima. \n",
        "\n",
        "2. El modelo sufre de OverFitting severo. El gap entre el `Train` y el `Val` sigue siendo muy grande, más del 30%. \n",
        "El cambio del `Loss` es también marginal. El `Val Accuracy` no mejora. El modelo ha memorizado el dataset y no generaliza.\n",
        "\n",
        "3. Con el enfoque actual, entrenando el modelo más epocas incrementaría el `OverFitting`.\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "\n",
        "<td width=\"100%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        " Después de esta segunda fase de entrenamiento -> en realidad el checkpoint sería como aplicar una pausa   \n",
        " y esta segunda fase de la casilla anterior, sería simplemente como quitarle el pausado. Se trata del mismo\n",
        " entrenamiento y sigue teniendo los mismos síntomas.\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "571fe3f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos del entrenamiento:\n",
            "Épocas: [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Train Accuracy: [79.5, 81.8, 82.4, 83.6, 82.2, 83.0, 82.4, 83.4]\n",
            "Validation Accuracy: [52.8, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
          ]
        }
      ],
      "source": [
        "# Datos extraídos de los outputs del entrenamiento (8 épocas totales)\n",
        "train_accuracies = [79.5, 81.8, 82.4, 83.6, 82.2, 83.0, 82.4, 83.4]\n",
        "validation_accuracies = [52.8, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n",
        "epochs_list = list(range(1, 9))  # Épocas 1-8\n",
        "\n",
        "print(\"Datos del entrenamiento:\")\n",
        "print(f\"Épocas: {epochs_list}\")\n",
        "print(f\"Train Accuracy: {train_accuracies}\")\n",
        "print(f\"Validation Accuracy: {validation_accuracies}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295d477e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_comparative(epochs, train_acc, val_acc, title=\"Training vs Validation Accuracy\"):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    \n",
        "    # Plotear ambas curvas\n",
        "    plt.plot(epochs, train_acc, 'r-o', label='Training Accuracy', linewidth=1, markersize=4)\n",
        "    plt.plot(epochs, val_acc, 'c-o', label='Validation Accuracy', linewidth=1, markersize=4)\n",
        "    \n",
        "    # Configuración del gráfico\n",
        "    plt.title(title, fontsize=10, fontweight='bold')\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "    plt.legend(fontsize=9)\n",
        "    plt.grid(True, alpha=0.4)\n",
        "    plt.ylim(30, 100)  # Rango (y) apropiado para nuestros datos\n",
        "        \n",
        "    # plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejecutar visualización\n",
        "plot_comparative(epochs_list, train_accuracies, validation_accuracies, \n",
        "                \"Overfitting Analysis: Training vs Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3db57d",
      "metadata": {
        "id": "df3db57d"
      },
      "source": [
        "## Ej 2. Problema de clasificación end-to-end (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3759a3a",
      "metadata": {
        "id": "d3759a3a"
      },
      "source": [
        "En esta última semana se va a poner en práctica todos los conocimientos adquiridos a lo largo de este módulo: deberás desarrollar un modelo neuronal para la clasificación de imágenes de frutas que ofrezca el mayor *accuracy* posible. No hay más indicaciones o limitaciones que las siguientes:\n",
        "- El dataset a utilizar será el siguiente: [https://www.kaggle.com/datasets/sshikamaru/fruit-recognition/data](https://www.kaggle.com/datasets/sshikamaru/fruit-recognition/data).\n",
        "- Has de reutilizar un modelo neuronal utilizando PyTorch, tú decides el grado de reutilización. Ideas:\n",
        "  - Modelos de PyTorch Hub. [https://pytorch.org/hub/](https://pytorch.org/hub/)\n",
        "  - Modelos de Torchvision. [https://pytorch.org/vision/0.13/models.html](https://pytorch.org/vision/0.13/models.html)\n",
        "\n",
        "Trata de explicar todos los pasos seguidos y las decisiones de diseño. Recuerda, a modo de guía, los pasos habituales:\n",
        "\n",
        "1. Adquisición de datos y carga en **Datasets**. Recomiendo aqui hacer un **EDA** para conocer bien la naturaleza del dataset.\n",
        "2. Creación de **DataLoaders**.\n",
        "3. Adaptación del modelo pre-entrenado.\n",
        "4. Definición de optimizador y función de coste apropiadas.\n",
        "5. Entrenamiento y validación.\n",
        "6. Análisis de métricas en test (las visualizaciones son bienvenidas)\n",
        "\n",
        "¡A por ello!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723bc64b",
      "metadata": {
        "id": "723bc64b"
      },
      "outputs": [],
      "source": [
        "# Tu respuesta a partir de aquí"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "102a4ceb",
        "owTz2_yBfoxA",
        "H2Z44bB4ksDp",
        "e1280bba",
        "2af82f39",
        "e89b5d71",
        "df3db57d"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
