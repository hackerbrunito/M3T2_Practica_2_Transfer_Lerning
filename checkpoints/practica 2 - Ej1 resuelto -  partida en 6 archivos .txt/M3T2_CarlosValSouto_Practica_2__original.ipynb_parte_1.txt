{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7cfb882",
      "metadata": {
        "id": "e7cfb882"
      },
      "source": [
        "# Práctica 2: Reutilización de grandes modelos neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b801bd8e",
      "metadata": {
        "id": "b801bd8e"
      },
      "source": [
        "En la segunda práctica del módulo vamos a utilizar la técnica más extendida para el uso productivo de redes neuronales: la **reutilización**, aka, **transfer learning**. Usaremos esta tecnica para dar solución al problema el cual hemos enfrentado en la práctica 1 sin mucho éxito. El dataset que vamos a utilizar es el mismo que en la Practica 1, que se puede encontrar [aquí](https://www.kaggle.com/datasets/anshtanwar/pets-facial-expression-dataset).\n",
        "\n",
        "La parte final de esta práctica tiene como objetivo enfrentar de principio a fin un caso de uso utilizando Redes Neuronales Convolucionales reutilizadas.\n",
        "\n",
        "La practica tiene **dos ejercicios**:\n",
        "\n",
        "1. Ej 1. Aprender a reutilizar modelos (5 puntos)\n",
        "2. Ej 2. Problema de clasificación end-to-end (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dz19wU6dZxRE",
      "metadata": {
        "id": "Dz19wU6dZxRE"
      },
      "source": [
        "### Objetivos\n",
        "\n",
        "Los objetivos esta primera práctica son los siguientes:\n",
        "- Reutilización de modelos y su adaptación a problemas concretos.\n",
        "- Utilización de técnicas de *data augmentation* utilizando PyTorch.\n",
        "- Utilización de modelos y pesos pre-entrenados de PyTorch.\n",
        "- Enfrentar un problema de reutilización desde el principio hasta el final.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jteHjSe-Zyz6",
      "metadata": {
        "id": "jteHjSe-Zyz6"
      },
      "source": [
        "### Consideraciones generales en el uso de Notebooks\n",
        "- Asegúrate de que todo el código necesario para ejecutarlo completamente está disponible.\n",
        "- Asegúrate de usar un orden lógico de ejecución de celdas para permitir una ejecución completa y automática. El Notebook debe poder funcionar perfectamente simplemente lanzando la opción \"Restart & Run All\" de la sección \"Kernel\". Cuidado con instanciar variables o métodos en celdas posteriores a las cuales están utilizadas, una ejecución independiente y manual puede hacer que funcione, pero la ejecución automática fallará.\n",
        "- Asegúrate de utilizar las celdas de tipo \"Markdown\" para texto y las de \"Code\" para código. Combinar distintos tipos de celda ayuda a la explicabilidad de la resolución y a la limpieza en general.\n",
        "- Puedes crear celdas adicionales si lo necesitas.\n",
        "- A la hora de la entrega del Notebook definitivo, asegúrate que lo entregas ejecutado y con las salidas de las celdas guardadas. Esto será necesario para la generación del PDF de entrega.\n",
        "- El código cuanto más comentado mejor. Además, cualquier reflexión adicional siempre será bienvenida."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102a4ceb",
      "metadata": {
        "id": "102a4ceb"
      },
      "source": [
        "## Ej 1. Aprender a reutilizar modelos (5 puntos)\n",
        "\n",
        "El objetivo de la práctica de esta semana será utilizar otra estrategia enfrentar el problema que venimos arrastrando de la primera actividad: la clasificación de emociones en imágenes de animales.\n",
        "\n",
        "Como pudiste observar, diseñar una red neuronal convencional y adaptar los datos para su uso resultó en un rendimiento muy pobre: no logró aprender y sus predicciones se daban practicamente al  azar. Posteriormente, utilizamos una arquitectura convencional, concretamente una AlexNet, para intentar mejorar el rendimiento. El resultado, de nuevo, dejó mucho que desear.\n",
        "\n",
        "No sé si te has parado a analizar el problema que estamos enfrentando. ¿Has visto las fotos de los animales? ¿Sabrías decir si los pájaros están tristes, contentos...? En definitiva, es un problema complicado de resolver. Además, como recordarás de la teoría, las redes neuronales exigen muchos datos para ser entrenadas, más aún si hay que enfrentar un problema complejo como este. ¿Cuál es la realidad?, que únicamente contamos con 1000 imágenes para entrenar... ya hemos visto que no son suficientes para entrenar redes neuronales.\n",
        "\n",
        "Llegados a este punto, existen dos opciones que pueden ser perfectamente complementarias:\n",
        "1.  **Obtener más datos**. El problema es que no siempre es sencillo (ni barato). Para lograr obtener más datos la solución alternativa pasa por utilizar técnicas de *data augmentation*. Por ejemplo, se pueden utilizar modelos generativos (desde GANs hasta modelos de difusion como DALLE-2), o utilizar modelos de Style Transfer para generar variantes estilísticas de una misma imagen. Una alternativa muy comun y facil de implementar para *data augmentation* consiste en realizar **transformaciones a las imagenes**. Con más datos, podremos nutrir nuestro conjunto de entrenamiento y, casi con total seguridad, mejoraremos el rendimiento de nuestras redes. El problema es, ¿cuántos datos son suficientes? La respuesta: muchos. Y más aún si queremos entrenar una red desde 0.\n",
        "\n",
        "2.  **Utilizar Transfer Learning**. Podemos reutilizar tanto la arquitectura como los pesos de un modelo entrenado con buenas métricas para adaptarlo a nuestro problema en concreto y, con eso, solventar parcialmente la falta de datos.\n",
        "\n",
        "En esta practica vamos a desarrollar **la segunda opción**, ya que suele ser la más viable en problemas \"reales\". Ademas, si queremos aplicar data augmentation usando transformaciones, necesitaremos entrenar el modelo mas epochs. Esto es porque PyTorch cambia el contenido de cada imagen en cada época, no aumenta el\n",
        "tamaño del dataset directamente. Dejo como **ejercicio voluntario** el intentar aumentar el conjunto de datos para mejorar aún más el rendimiento de alguno de los modelos, teniendo en cuenta que habra que entrenar durante mas epochs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owTz2_yBfoxA",
      "metadata": {
        "id": "owTz2_yBfoxA"
      },
      "source": [
        "### Importar las librerias necesarias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3dcf7d2b",
      "metadata": {
        "id": "3dcf7d2b"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "06d7be32",
      "metadata": {
        "id": "06d7be32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H2Z44bB4ksDp",
      "metadata": {
        "id": "H2Z44bB4ksDp"
      },
      "source": [
        "### Ej 1.1: Carga de datos y modelo (0.5 puntos)\n",
        "\n",
        "Los datos que vamos a utilizar serán los mismos que en la Práctica 1, que los tienes disponibles [aqui](https://www.kaggle.com/datasets/anshtanwar/pets-facial-expression-dataset). Como opción alternativa a la descarga en local, se pueden tambien cargar en cache como se ofreció en la Practica 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Yy0z0Ecxktfw",
      "metadata": {
        "id": "Yy0z0Ecxktfw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"anshtanwar/pets-facial-expression-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9630e88",
      "metadata": {
        "id": "a9630e88"
      },
      "source": [
        "\n",
        "La diferencia reside en que en la Práctica 1, las transformaciones aplicadas sobre las imágenes en bruto (variable *transformations*) era diseñada por nosotros. En este caso, dado que vamos a utilizar un modelo pre-entrenado, habrá que adaptarse al formato de dato que espera el modelo para unos pesos en concreto. Dependiendo del diseño de la fuente de donde se carguen los modelos, las transformaciones se pueden obtener de distinta forma.\n",
        "\n",
        "En este caso, vamos a hacer uso de los modelos que Torchvision ofrece de forma nativa, que tenéis disponibles [aqui](https://pytorch.org/vision/stable/models.html). Concretamente, vamos a utilizar:\n",
        "\n",
        "- Un modelo **Efficient Net B1**.\n",
        "- Como pesos, la segunda versión (v2) de los entrenados sobre IMAGENET1K.\n",
        "\n",
        "En el caso de Torchvision, las transformaciones se pueden localizar en el atributo *transforms* de los pesos. Para recuperar el pipeline completo de pre-procesamiento, se puede hacer uso de `weights.transforms()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd2c3622",
      "metadata": {
        "id": "fd2c3622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado: EfficientNet B1 - v2\n",
            "Pesos utilizados: EfficientNet_B1_Weights.IMAGENET1K_V2\n",
            "Transformaciones obtenidas automáticamente:\n",
            "\n",
            " ImageClassification(\n",
            "    crop_size=[240]\n",
            "    resize_size=[255]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importar los pesos específicos de EfficientNet B1 v2\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "weights = EfficientNet_B1_Weights.IMAGENET1K_V2 # Completar\n",
        "model = efficientnet_b1(weights=weights) # Completar\n",
        "transformation = weights.transforms()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Modelo cargado: EfficientNet B1 - v2\")\n",
        "print(f\"Pesos utilizados: {weights}\")\n",
        "print(f\"Transformaciones obtenidas automáticamente:\\n\\n {transformation}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "10bd1f2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Angry/\n",
            "=> happy/\n",
            "=> Master Folder/\n",
            "     => test/\n",
            "     => train/\n",
            "     => valid/\n",
            "=> Other/\n",
            "=> Sad/\n"
          ]
        }
      ],
      "source": [
        "# No incluido en la práctic 2\n",
        "# Listado de los directorios del dtaset\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_base = r\"C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\"\n",
        "\n",
        "for item in os.listdir(dataset_base):\n",
        "   item_path = os.path.join(dataset_base, item)\n",
        "   if os.path.isdir(item_path):\n",
        "       print(f\"=> {item}/\")\n",
        "       # Ver subdirectorios\n",
        "       for sub_item in os.listdir(item_path):\n",
        "           sub_path = os.path.join(item_path, sub_item)\n",
        "           if os.path.isdir(sub_path):\n",
        "               print(f\"     => {sub_item}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4419e751",
      "metadata": {
        "id": "4419e751"
      },
      "source": [
        "Ya podemos entonces crear las instancias de **Dataset** para los tres conjuntos: `train_dataset`, `validation_dataset` y `test_dataset`. ¡No olvides aplicar las transformaciones recuperadas anteriormente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f8e3dd06",
      "metadata": {
        "id": "f8e3dd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset de entrenamiento: 1000 imágenes.\n",
            "Dataset de validación: 36 imágenes.\n",
            "Dataset de test: 38 imágenes.\n",
            "El dataset contiene '4' clases  > >> ['Angry', 'Other', 'Sad', 'happy'] \n"
          ]
        }
      ],
      "source": [
        "# Path de kagglehub para la ubicación de los datos.\n",
        "data_path = r\"C:\\Users\\Carlos\\.cache\\kagglehub\\datasets\\anshtanwar\\pets-facial-expression-dataset\\versions\\11\\Master Folder\"\n",
        "\n",
        "# Crear los datasets con las transformaciones del modelo pre-entrenado\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'train'),\n",
        "    transform=transformation\n",
        ")\n",
        "validation_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'valid'),\n",
        "    transform=transformation\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(data_path, 'test'),\n",
        "    transform=transformation\n",
        ")\n",
        "\n",
        "print(f\"Dataset de entrenamiento: {len(train_dataset)} imágenes.\")\n",
        "print(f\"Dataset de validación: {len(validation_dataset)} imágenes.\")\n",
        "print(f\"Dataset de test: {len(test_dataset)} imágenes.\")\n",
        "print(f\"El dataset contiene '{len(train_dataset.classes)}' clases  > >> {train_dataset.classes} \")\n",
        "# print(f\"Numero de clases: {len(train_dataset.classes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abec152",
      "metadata": {
        "id": "3abec152"
      },
      "source": [
        "Creamos los **DataLoader** correspondientes a cada **Dataset**: `train_dataloader`, `validation_dataloader` y `test_dataloader`. Utilizaremos para todos ellos `batch_size` de 64, con el objetivo de agilizar el entrenamiento, y mezcla en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e6fef3b",
      "metadata": {
        "id": "2e6fef3b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b71a966",
      "metadata": {
        "id": "3b71a966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoaders creados:\n",
            "- Train: 16 batches de 64\n",
            "- Validation: 1 batches de 64\n",
            "- Test: 1 batches de 64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configurar parámetros de carga\n",
        "batch_size = 64  # Tamaño de batch \n",
        "num_workers = 4  # Núcleos de CPU para cargar datos en paralelo\n",
        "\n",
        "# Crear los DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,        # Mezclar datos de entrenamiento\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True      # Optimización para GPU\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,       # No mezclar validación (orden no importa)\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,       # No mezclar test\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders creados:\")\n",
        "print(f\"- Train: {len(train_dataloader)} batches de {batch_size}\")\n",
        "print(f\"- Validation: {len(validation_dataloader)} batches de {batch_size}\")\n",
        "print(f\"- Test: {len(test_dataloader)} batches de {batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1280bba",
      "metadata": {
        "id": "e1280bba"
      },
      "source": [
        "### Ej 1.2: Adaptación del modelo (2 puntos)\n",
        "\n",
        "Los pesos que hemos seleccionado han sido entrenados utilizando IMAGENET en su versión con 1000 etiquetas. Nuestro problema es diferente, por lo que tendremos que adaptar el modelo.\n",
        "\n",
        "El objetivo será \"congelar\" toda la parte convolucional del modelo, es decir, el extractor de características. Congelar implica que los parámetros no se entrenen, por dos motivos:\n",
        "- No queremos \"ensuciar\" el entrenamiento que se ha realizado.\n",
        "- No tenemos recursos suficientes para entrenar una red con este número de parámetros.\n",
        "\n",
        "Posteriormente, modificaremos el clasificador para adaptar el modelo a nuestro problema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5c6a51",
      "metadata": {
        "id": "6c5c6a51"
      },
      "source": [
        "Muestra la arquitectura del modelo cargado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2ebab975",
      "metadata": {
        "id": "2ebab975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquitectura completa del modelo EfficientNet B1:\n",
            "EfficientNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2dNormActivation(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (2): Conv2dNormActivation(\n",
            "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
            "      )\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): SqueezeExcitation(\n",
            "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (activation): SiLU(inplace=True)\n",
            "            (scale_activation): Sigmoid()\n",
            "          )\n",
            "          (2): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2dNormActivation(\n",
            "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv2dNormActivation(\n",
            "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): SiLU(inplace=True)\n",
            "          )\n",
            "  