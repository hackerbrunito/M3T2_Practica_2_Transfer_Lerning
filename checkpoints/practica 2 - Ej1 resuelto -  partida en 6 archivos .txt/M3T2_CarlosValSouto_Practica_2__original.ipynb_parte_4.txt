 de la capa Linear:\n",
            "Entrada: 1280\n",
            "Salida: 4\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n"
          ]
        }
      ],
      "source": [
        "# Verificar que se ha actualizado correctamente\n",
        "print(\"^\" * 60)\n",
        "print(\"   CLASIFICADOR ACTUALIZADO \")\n",
        "print(\"^\" * 60)\n",
        "\n",
        "print(f\"Nuevo clasificador:\")\n",
        "print(model.classifier)\n",
        "print(\"¨\" * 60)\n",
        "\n",
        "print(f\"\\nDimensiones de la capa Linear:\")\n",
        "print(f\"Entrada: {model.classifier[1].in_features}\")\n",
        "print(f\"Salida: {model.classifier[1].out_features}\")\n",
        "print(\"¨\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12fef29",
      "metadata": {},
      "source": [
        "### Comentario del código anterior\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "\n",
        "Con anterioridad a este paso, obtuvimos que:\n",
        "    \n",
        "(0): Dropout(p=0.2, inplace=True)\n",
        "\n",
        "(1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "\n",
        "\n",
        "\n",
        "</td>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "Después de actualizar de clasificador tenemos:\n",
        "  \n",
        "(0): Dropout(p=0.3, inplace=True)\n",
        "  \n",
        "(1): Linear(in_features=1280, out_features=4, bias=True)\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af82f39",
      "metadata": {
        "id": "2af82f39"
      },
      "source": [
        "### Ej 1.3: Checkpoints (1.5 puntos)\n",
        "\n",
        "Ya tenemos todo listo para poder entrenar nuestro modelo. El único problema es que el entrenamiento va a ser costoso (aunque podría serlo muchísimo más), por lo que lo correcto es utilizar *checkpoints* para evitar cualquier inconveniente que pueda ocurrir durante el entrenamiento que haga que el entrenamiento se detenga y, por tanto, echar a perder todo el cómputo realizado.\n",
        "\n",
        "Es por ello que antes que nada vamos a crear dos funciones que permitan guardar y cargar un *checkpoint* utilizando las herramientas que nos proporciona PyTorch. Recuerda guardar todo lo que sea necesario para que el modelo pueda ser recuperado íntegramente o, al menos, que pueda seguir entrenando por donde iba.\n",
        "\n",
        "En PyTorch, el formato para guardar *checkpoints* es *.pth*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974e9cdc",
      "metadata": {
        "id": "974e9cdc"
      },
      "source": [
        "#### Método para guardar un *checkpoint*\n",
        "- **path**: Dirección donde se va a almacenar el *checkpoint*\n",
        "- **model**: Instancia del modelo a almacenar\n",
        "- **optimizer**: Optimizador utilizado, con el objetivo de guardar su estado (recuerda que hay optimizadores con hiperparámetros progresivos).\n",
        "\n",
        "El método no devuelve nada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c38c901e",
      "metadata": {
        "id": "c38c901e"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, model, optimizer):\n",
        "    \n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint guardado en: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02005825",
      "metadata": {
        "id": "02005825"
      },
      "source": [
        "#### Método para recuperar un *checkpoint*\n",
        "- **path**: Dirección donde se va a recuperar el *checkpoint*\n",
        "- **model**: Instancia \"vacía\" del modelo recuperado. La arquitectura debe ser la misma que a la hora del guardado.\n",
        "- **optimizer**: Instancia nueva del optimizador recuperado. Debe ser del mismo tipo para poder actualizar su estado.\n",
        "\n",
        "El método debe devolver:\n",
        "- **model**: Modelo actualizado.\n",
        "- **optimizer**: Optimizador actualizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "068912e5",
      "metadata": {
        "id": "068912e5"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    print(f\"Checkpoint cargado desde: {path}\")\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89b5d71",
      "metadata": {
        "id": "e89b5d71"
      },
      "source": [
        "### Ej 1.4: Entrenamiento del modelo (1.0 punto)\n",
        "\n",
        "Finalmente, ya tenemos toda la información y herramientas necesarias para proceder a entrenar el modelo. A continuacion, se ofrecen los metodos para entrenar y evaluar los conjuntos de validation/test que ya se trabajaron en la Practica 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aeff4d51",
      "metadata": {
        "id": "aeff4d51"
      },
      "outputs": [],
      "source": [
        "def training(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    model.train() # Indicamos al modelo que vamos a entrenar\n",
        "\n",
        "    num_batches = len(dataloader) # Número de batches para promediar el loss\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader): # Iteramos por los distintos batches creados en el dataloader\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X) # Calculamos el forward pass\n",
        "        loss = loss_fn(pred, y) # Calculamos el loss\n",
        "\n",
        "        _,predicted=torch.max(pred,1)\n",
        "        correct+=(y==predicted).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # Limpiamos los gradientes viejos\n",
        "        loss.backward() # Calculamos los nuevos gradientes\n",
        "        optimizer.step() # Actualizamos los parámetros (pesos) del modelo\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    correct /= size\n",
        "    avg_train_loss = train_loss/num_batches\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Train Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg train loss: {avg_train_loss:>8f}\\n\")\n",
        "    return avg_train_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6c5cc412",
      "metadata": {
        "id": "6c5cc412"
      },
      "outputs": [],
      "source": [
        "def test_and_validation(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() # Indicamos que no vamos a entrenar\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            _,predicted=torch.max(pred,1)\n",
        "            correct+=(y==predicted).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    avg_loss = test_loss\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Val or Test Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934dd291",
      "metadata": {
        "id": "934dd291"
      },
      "source": [
        "Utilizaremos **Adam** como optimizador y **CrossEntropyLoss** como función de coste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "199f43fd",
      "metadata": {
        "id": "199f43fd"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c4e96e",
      "metadata": {
        "id": "03c4e96e"
      },
      "source": [
        "Entrena durante **4 épocas** el modelo guardando al final de cada una de ellas un *checkpoint*. Puedes reescribir siempre el mismo o crear un *checkpoint* para cada una de las épocas.\n",
        "\n",
        "**Atención**: Para simplemente probar el código te recomiendo que pruebes únicamente con una época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "72b175b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponible: True\n",
            "Device actual: cuda\n",
            "Modelo en device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# verificando que el \"device\" es --->> cuda\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "print(f\"Device actual: {device}\")\n",
        "print(f\"Modelo en device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "daf16d1c",
      "metadata": {
        "id": "daf16d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 69.1%, Avg train loss: 0.934270\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.236083 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_1.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 2\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 70.5%, Avg train loss: 0.913130\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 52.8%, Avg loss: 1.234703 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_2.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 3\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 71.8%, Avg train loss: 0.889898\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.235596 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_3.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 4\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 73.3%, Avg train loss: 0.868803\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.238840 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_4.pth\n",
            "\n",
            "-------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 4\n",
        "for t in range(epochs):\n",
        "    print(f\"\\n-------------------------------\\n  ==> Epoch - {t+1}\\n-------------------------------\\n\")\n",
        "    \n",
        "    # Entrenar\n",
        "    training(train_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    # Validar\n",
        "    test_and_validation(validation_dataloader, model, loss_fn)\n",
        "    \n",
        "    # Guardar checkpoint al final de cada época\n",
        "    save_checkpoint(f\"checkpoint_epoch_{t+1}.pth\", model, optimizer)\n",
        "    \n",
        "    print(f\"\\n-------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e476bde",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "Tras repetidas ejecuciones, 5 ciclos consecutivos del entrenamiento han dado estos outputs:\n",
        "\n",
        "\n",
        "<table> <tr> <td>  **Train Accuracy**  \n",
        "  \n",
        "**Ejecución 1**: 36.5% → 72.1%  \n",
        "**Ejecución 2**: 72.8% → 76.3%   \n",
        "**Ejecución 3**: 76.3% → 77.8%  \n",
        "**Ejecución 4**: 80.2% → 78.5%  \n",
        "**Ejecución 5**: 79.5% → 83.6%  \n",
        "\n",
        "\n",
        "</td> <td> **Validation Accuracy**  \n",
        "  \n",
        "**Ejecución 1**: 41.7% → 52.8%  \n",
        "**Ejecución 2**: 52.8% → 52.8%  \n",
        "**Ejecución 3**: 52.8% → 52.8%  \n",
        "**Ejecución 4**: 52.8% → 50.0%  \n",
        "**Ejecución 5**: 52.8% → 50.0%  \n",
        "\n",
        "</td> <td> **Validation Loss**  \n",
        "  \n",
        "**Ejecución 1**: 1.309 → 1.243  \n",
        "**Ejecución 2**: 1.227 → 1.226  \n",
        "**Ejecución 3**: 1.227 → 1.236  \n",
        "**Ejecución 4**: 1.246 → 1.243  \n",
        "**Ejecución 5**: 1.255 → 1.274  \n",
        "\n",
        "</td> </tr> </table>\n",
        "\n",
        "<table> <tr> \n",
        "\n",
        "\"mensaje después de terminar el ejercicio 1.4.\"\n",
        "\n",
        " Al final del ejercicio 1.4 y después de escribir todos los comentarios de todas las casillas.\n",
        "- he reiniciado el kernel y por lo tanto el resultado del training despues de reiniar no cuadra no cuadra con la última ejecución de la primera tabla 😂😂.. \n",
        "\n",
        "añado los valores obtenidos despues de reiniciar el kernel </tr>\n",
        "\n",
        "<tr><td>  **Train Accuracy**  \n",
        "\n",
        "Época 1: 69.1%  \n",
        "Época 2: 70.5%  \n",
        "Época 3: 71.8%  \n",
        "Época 4: 73.3%  \n",
        "</td><td> **Validation Accuracy**  \n",
        "\n",
        "Época 1: 50.0%  \n",
        "Época 2: 52.8%  \n",
        "Época 3: 50.0%  \n",
        "Época 4: 50.0%  \n",
        "</td><td> **Validation Loss**  \n",
        "\n",
        "Época 1: 1.236  \n",
        "Época 2: 1.235  \n",
        "Época 3: 1.236  \n",
        "Época 4: 1.239  \n",
        "</td></tr></table>\n",
        "\n",
        "Observaciones:\n",
        "  \n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "- **Train Accuracy**: Obtiene una mejora progresiva de más del 70%\n",
        "- **Validation Accuracy**: Muestra un estancamiento sobre ~50-53%\n",
        "- **Validation Loss**: Se mantiene estable ~1.23-1.27\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "El estancamiento del `Validation` nos da a entender que hay `OverFitting`. Un resultado óptimo sería cuando: \n",
        "\n",
        "\n",
        "- El Accuracy de `Training` y `Validation` tuviera en ambos una subida progresiva y mantuviera un \"gap\" del 10-15% o menos de diferencia.\n",
        "- El `Loss` de Train y Validation tuviera en ambos una bajada progresiva.\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e48d71",
      "metadata": {
        "id": "67e48d71"
      },
      "source": [
        "Ahora, vamos a utilizar los **checkpoints** para entrenar el modelo mas epochs.\n",
        "\n",
        "Carga el último **checkpoint** generado en la útima época del último entrenamiento en las nuevas variables `model_recovered` y `optimizer_recovered` para entrenar el modelo **4 épocas más**.\n",
        "\n",
        "Consideraciones:\n",
        "- El loss en test de la última época del primer entrenamiento **debe coincidir** con el loss en test antes de la primera época del segundo entrenamiento. Muestra ambos valores para verificar que, efectivamente, estás entrenando el \"mismo\" modelo.\n",
        "- Para cargar un *checkpoint* la arquitectura del modelo debe ser la misma que a la hora de guardarlo. Si mantienes la misma sesion de colab y tienes el modelo **Efficient Net B1** correctamnte adaptado y en la variable `model`, puedes usar esta variable para cargar el checkpoint. Si este no es el caso, deberás crear una instancia nueva del Efficient Net y repetir el proceso seguido en el ejercicio 1.2 para adaptar su arquitectura y, con ello, poder cargar los pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f60e9245",
      "metadata": {
        "id": "f60e9245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Carlos\\AppData\\Local\\Temp\\ipykernel_39620\\3128740935.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint cargado desde: checkpoint_epoch_4.pth\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n",
            "Evaluando modelo cargado ANTES del segundo entrenamiento:\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.238840 \n",
            "\n",
            "¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
        "# Cargar el último checkpoint en nuevas variables\n",
        "print(\"¨\" * 60 + \"\\n\")\n",
        "model_recovered = model  # Reutilizar modelo existente (misma sesión)\n",
        "\n",
        "# Crear optimizador IGUAL al original (solo classifier.parameters)\n",
        "optimizer_recovered = torch.optim.Adam(model_recovered.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Cargar el último checkpoint\n",
        "checkpoint = load_checkpoint(\"checkpoint_epoch_4.pth\", model_recovered, optimizer_recovered)\n",
        "\n",
        "print(\"¨\" * 60 + \"\\n\")\n",
        "\n",
        "print(\"Evaluando modelo cargado ANTES del segundo entrenamiento:\")\n",
        "print()\n",
        "test_and_validation(validation_dataloader, model_recovered, loss_fn)\n",
        "print(\"¨\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72a006c",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "\n",
        "`Loss` = 1.238840  ->  ambos coinciden (época 4 del entrenamiento anterior y checkpoit cargado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455f4959",
      "metadata": {
        "id": "455f4959"
      },
      "source": [
        "Ahora podemos proceder con el re-entrenamiento con el modelo recien cargado usando el checkpoint. Implementa el codigo para entrenar **4 epochs mas** el modelo en la variable `model_recovered`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "444bbd43",
      "metadata": {
        "id": "444bbd43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 74.0%, Avg train loss: 0.839671\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.241052 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_1.pth\n",
            "\n"