 de la capa Linear:\n",
            "Entrada: 1280\n",
            "Salida: 4\n",
            "Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨\n"
          ]
        }
      ],
      "source": [
        "# Verificar que se ha actualizado correctamente\n",
        "print(\"^\" * 60)\n",
        "print(\"   CLASIFICADOR ACTUALIZADO \")\n",
        "print(\"^\" * 60)\n",
        "\n",
        "print(f\"Nuevo clasificador:\")\n",
        "print(model.classifier)\n",
        "print(\"Â¨\" * 60)\n",
        "\n",
        "print(f\"\\nDimensiones de la capa Linear:\")\n",
        "print(f\"Entrada: {model.classifier[1].in_features}\")\n",
        "print(f\"Salida: {model.classifier[1].out_features}\")\n",
        "print(\"Â¨\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12fef29",
      "metadata": {},
      "source": [
        "### Comentario del cÃ³digo anterior\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "\n",
        "Con anterioridad a este paso, obtuvimos que:\n",
        "    \n",
        "(0): Dropout(p=0.2, inplace=True)\n",
        "\n",
        "(1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "\n",
        "\n",
        "\n",
        "</td>\n",
        "\n",
        "<td width=\"50%\" style=\"padding: 20px; vertical-align: top;\">\n",
        "\n",
        "DespuÃ©s de actualizar de clasificador tenemos:\n",
        "  \n",
        "(0): Dropout(p=0.3, inplace=True)\n",
        "  \n",
        "(1): Linear(in_features=1280, out_features=4, bias=True)\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af82f39",
      "metadata": {
        "id": "2af82f39"
      },
      "source": [
        "### Ej 1.3: Checkpoints (1.5 puntos)\n",
        "\n",
        "Ya tenemos todo listo para poder entrenar nuestro modelo. El Ãºnico problema es que el entrenamiento va a ser costoso (aunque podrÃ­a serlo muchÃ­simo mÃ¡s), por lo que lo correcto es utilizar *checkpoints* para evitar cualquier inconveniente que pueda ocurrir durante el entrenamiento que haga que el entrenamiento se detenga y, por tanto, echar a perder todo el cÃ³mputo realizado.\n",
        "\n",
        "Es por ello que antes que nada vamos a crear dos funciones que permitan guardar y cargar un *checkpoint* utilizando las herramientas que nos proporciona PyTorch. Recuerda guardar todo lo que sea necesario para que el modelo pueda ser recuperado Ã­ntegramente o, al menos, que pueda seguir entrenando por donde iba.\n",
        "\n",
        "En PyTorch, el formato para guardar *checkpoints* es *.pth*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974e9cdc",
      "metadata": {
        "id": "974e9cdc"
      },
      "source": [
        "#### MÃ©todo para guardar un *checkpoint*\n",
        "- **path**: DirecciÃ³n donde se va a almacenar el *checkpoint*\n",
        "- **model**: Instancia del modelo a almacenar\n",
        "- **optimizer**: Optimizador utilizado, con el objetivo de guardar su estado (recuerda que hay optimizadores con hiperparÃ¡metros progresivos).\n",
        "\n",
        "El mÃ©todo no devuelve nada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c38c901e",
      "metadata": {
        "id": "c38c901e"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, model, optimizer):\n",
        "    \n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint guardado en: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02005825",
      "metadata": {
        "id": "02005825"
      },
      "source": [
        "#### MÃ©todo para recuperar un *checkpoint*\n",
        "- **path**: DirecciÃ³n donde se va a recuperar el *checkpoint*\n",
        "- **model**: Instancia \"vacÃ­a\" del modelo recuperado. La arquitectura debe ser la misma que a la hora del guardado.\n",
        "- **optimizer**: Instancia nueva del optimizador recuperado. Debe ser del mismo tipo para poder actualizar su estado.\n",
        "\n",
        "El mÃ©todo debe devolver:\n",
        "- **model**: Modelo actualizado.\n",
        "- **optimizer**: Optimizador actualizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "068912e5",
      "metadata": {
        "id": "068912e5"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "    if not path.endswith(\".pth\"):\n",
        "        print(\"El checkpoint debe tener formato .pth\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    print(f\"Checkpoint cargado desde: {path}\")\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89b5d71",
      "metadata": {
        "id": "e89b5d71"
      },
      "source": [
        "### Ej 1.4: Entrenamiento del modelo (1.0 punto)\n",
        "\n",
        "Finalmente, ya tenemos toda la informaciÃ³n y herramientas necesarias para proceder a entrenar el modelo. A continuacion, se ofrecen los metodos para entrenar y evaluar los conjuntos de validation/test que ya se trabajaron en la Practica 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aeff4d51",
      "metadata": {
        "id": "aeff4d51"
      },
      "outputs": [],
      "source": [
        "def training(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    model.train() # Indicamos al modelo que vamos a entrenar\n",
        "\n",
        "    num_batches = len(dataloader) # NÃºmero de batches para promediar el loss\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader): # Iteramos por los distintos batches creados en el dataloader\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X) # Calculamos el forward pass\n",
        "        loss = loss_fn(pred, y) # Calculamos el loss\n",
        "\n",
        "        _,predicted=torch.max(pred,1)\n",
        "        correct+=(y==predicted).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # Limpiamos los gradientes viejos\n",
        "        loss.backward() # Calculamos los nuevos gradientes\n",
        "        optimizer.step() # Actualizamos los parÃ¡metros (pesos) del modelo\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    correct /= size\n",
        "    avg_train_loss = train_loss/num_batches\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Train Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg train loss: {avg_train_loss:>8f}\\n\")\n",
        "    return avg_train_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6c5cc412",
      "metadata": {
        "id": "6c5cc412"
      },
      "outputs": [],
      "source": [
        "def test_and_validation(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() # Indicamos que no vamos a entrenar\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            _,predicted=torch.max(pred,1)\n",
        "            correct+=(y==predicted).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    avg_loss = test_loss\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Val or Test Metrics: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934dd291",
      "metadata": {
        "id": "934dd291"
      },
      "source": [
        "Utilizaremos **Adam** como optimizador y **CrossEntropyLoss** como funciÃ³n de coste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "199f43fd",
      "metadata": {
        "id": "199f43fd"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c4e96e",
      "metadata": {
        "id": "03c4e96e"
      },
      "source": [
        "Entrena durante **4 Ã©pocas** el modelo guardando al final de cada una de ellas un *checkpoint*. Puedes reescribir siempre el mismo o crear un *checkpoint* para cada una de las Ã©pocas.\n",
        "\n",
        "**AtenciÃ³n**: Para simplemente probar el cÃ³digo te recomiendo que pruebes Ãºnicamente con una Ã©poca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "72b175b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponible: True\n",
            "Device actual: cuda\n",
            "Modelo en device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# verificando que el \"device\" es --->> cuda\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "print(f\"Device actual: {device}\")\n",
        "print(f\"Modelo en device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "daf16d1c",
      "metadata": {
        "id": "daf16d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 69.1%, Avg train loss: 0.934270\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.236083 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_1.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 2\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 70.5%, Avg train loss: 0.913130\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 52.8%, Avg loss: 1.234703 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_2.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 3\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 71.8%, Avg train loss: 0.889898\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.235596 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_3.pth\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 4\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 73.3%, Avg train loss: 0.868803\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.238840 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_epoch_4.pth\n",
            "\n",
            "-------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 4\n",
        "for t in range(epochs):\n",
        "    print(f\"\\n-------------------------------\\n  ==> Epoch - {t+1}\\n-------------------------------\\n\")\n",
        "    \n",
        "    # Entrenar\n",
        "    training(train_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    # Validar\n",
        "    test_and_validation(validation_dataloader, model, loss_fn)\n",
        "    \n",
        "    # Guardar checkpoint al final de cada Ã©poca\n",
        "    save_checkpoint(f\"checkpoint_epoch_{t+1}.pth\", model, optimizer)\n",
        "    \n",
        "    print(f\"\\n-------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e476bde",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "Tras repetidas ejecuciones, 5 ciclos consecutivos del entrenamiento han dado estos outputs:\n",
        "\n",
        "\n",
        "<table> <tr> <td>  **Train Accuracy**  \n",
        "  \n",
        "**EjecuciÃ³n 1**: 36.5% â†’ 72.1%  \n",
        "**EjecuciÃ³n 2**: 72.8% â†’ 76.3%   \n",
        "**EjecuciÃ³n 3**: 76.3% â†’ 77.8%  \n",
        "**EjecuciÃ³n 4**: 80.2% â†’ 78.5%  \n",
        "**EjecuciÃ³n 5**: 79.5% â†’ 83.6%  \n",
        "\n",
        "\n",
        "</td> <td> **Validation Accuracy**  \n",
        "  \n",
        "**EjecuciÃ³n 1**: 41.7% â†’ 52.8%  \n",
        "**EjecuciÃ³n 2**: 52.8% â†’ 52.8%  \n",
        "**EjecuciÃ³n 3**: 52.8% â†’ 52.8%  \n",
        "**EjecuciÃ³n 4**: 52.8% â†’ 50.0%  \n",
        "**EjecuciÃ³n 5**: 52.8% â†’ 50.0%  \n",
        "\n",
        "</td> <td> **Validation Loss**  \n",
        "  \n",
        "**EjecuciÃ³n 1**: 1.309 â†’ 1.243  \n",
        "**EjecuciÃ³n 2**: 1.227 â†’ 1.226  \n",
        "**EjecuciÃ³n 3**: 1.227 â†’ 1.236  \n",
        "**EjecuciÃ³n 4**: 1.246 â†’ 1.243  \n",
        "**EjecuciÃ³n 5**: 1.255 â†’ 1.274  \n",
        "\n",
        "</td> </tr> </table>\n",
        "\n",
        "<table> <tr> \n",
        "\n",
        "\"mensaje despuÃ©s de terminar el ejercicio 1.4.\"\n",
        "\n",
        " Al final del ejercicio 1.4 y despuÃ©s de escribir todos los comentarios de todas las casillas.\n",
        "- he reiniciado el kernel y por lo tanto el resultado del training despues de reiniar no cuadra no cuadra con la Ãºltima ejecuciÃ³n de la primera tabla ðŸ˜‚ðŸ˜‚.. \n",
        "\n",
        "aÃ±ado los valores obtenidos despues de reiniciar el kernel </tr>\n",
        "\n",
        "<tr><td>  **Train Accuracy**  \n",
        "\n",
        "Ã‰poca 1: 69.1%  \n",
        "Ã‰poca 2: 70.5%  \n",
        "Ã‰poca 3: 71.8%  \n",
        "Ã‰poca 4: 73.3%  \n",
        "</td><td> **Validation Accuracy**  \n",
        "\n",
        "Ã‰poca 1: 50.0%  \n",
        "Ã‰poca 2: 52.8%  \n",
        "Ã‰poca 3: 50.0%  \n",
        "Ã‰poca 4: 50.0%  \n",
        "</td><td> **Validation Loss**  \n",
        "\n",
        "Ã‰poca 1: 1.236  \n",
        "Ã‰poca 2: 1.235  \n",
        "Ã‰poca 3: 1.236  \n",
        "Ã‰poca 4: 1.239  \n",
        "</td></tr></table>\n",
        "\n",
        "Observaciones:\n",
        "  \n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "- **Train Accuracy**: Obtiene una mejora progresiva de mÃ¡s del 70%\n",
        "- **Validation Accuracy**: Muestra un estancamiento sobre ~50-53%\n",
        "- **Validation Loss**: Se mantiene estable ~1.23-1.27\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>\n",
        "\n",
        "El estancamiento del `Validation` nos da a entender que hay `OverFitting`. Un resultado Ã³ptimo serÃ­a cuando: \n",
        "\n",
        "\n",
        "- El Accuracy de `Training` y `Validation` tuviera en ambos una subida progresiva y mantuviera un \"gap\" del 10-15% o menos de diferencia.\n",
        "- El `Loss` de Train y Validation tuviera en ambos una bajada progresiva.\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e48d71",
      "metadata": {
        "id": "67e48d71"
      },
      "source": [
        "Ahora, vamos a utilizar los **checkpoints** para entrenar el modelo mas epochs.\n",
        "\n",
        "Carga el Ãºltimo **checkpoint** generado en la Ãºtima Ã©poca del Ãºltimo entrenamiento en las nuevas variables `model_recovered` y `optimizer_recovered` para entrenar el modelo **4 Ã©pocas mÃ¡s**.\n",
        "\n",
        "Consideraciones:\n",
        "- El loss en test de la Ãºltima Ã©poca del primer entrenamiento **debe coincidir** con el loss en test antes de la primera Ã©poca del segundo entrenamiento. Muestra ambos valores para verificar que, efectivamente, estÃ¡s entrenando el \"mismo\" modelo.\n",
        "- Para cargar un *checkpoint* la arquitectura del modelo debe ser la misma que a la hora de guardarlo. Si mantienes la misma sesion de colab y tienes el modelo **Efficient Net B1** correctamnte adaptado y en la variable `model`, puedes usar esta variable para cargar el checkpoint. Si este no es el caso, deberÃ¡s crear una instancia nueva del Efficient Net y repetir el proceso seguido en el ejercicio 1.2 para adaptar su arquitectura y, con ello, poder cargar los pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f60e9245",
      "metadata": {
        "id": "f60e9245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Carlos\\AppData\\Local\\Temp\\ipykernel_39620\\3128740935.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint cargado desde: checkpoint_epoch_4.pth\n",
            "Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨\n",
            "\n",
            "Evaluando modelo cargado ANTES del segundo entrenamiento:\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.238840 \n",
            "\n",
            "Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨Â¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
        "# Cargar el Ãºltimo checkpoint en nuevas variables\n",
        "print(\"Â¨\" * 60 + \"\\n\")\n",
        "model_recovered = model  # Reutilizar modelo existente (misma sesiÃ³n)\n",
        "\n",
        "# Crear optimizador IGUAL al original (solo classifier.parameters)\n",
        "optimizer_recovered = torch.optim.Adam(model_recovered.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Cargar el Ãºltimo checkpoint\n",
        "checkpoint = load_checkpoint(\"checkpoint_epoch_4.pth\", model_recovered, optimizer_recovered)\n",
        "\n",
        "print(\"Â¨\" * 60 + \"\\n\")\n",
        "\n",
        "print(\"Evaluando modelo cargado ANTES del segundo entrenamiento:\")\n",
        "print()\n",
        "test_and_validation(validation_dataloader, model_recovered, loss_fn)\n",
        "print(\"Â¨\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72a006c",
      "metadata": {},
      "source": [
        "## Comentario de la casilla anterior\n",
        "\n",
        "`Loss` = 1.238840  ->  ambos coinciden (Ã©poca 4 del entrenamiento anterior y checkpoit cargado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455f4959",
      "metadata": {
        "id": "455f4959"
      },
      "source": [
        "Ahora podemos proceder con el re-entrenamiento con el modelo recien cargado usando el checkpoint. Implementa el codigo para entrenar **4 epochs mas** el modelo en la variable `model_recovered`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "444bbd43",
      "metadata": {
        "id": "444bbd43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------\n",
            "  ==> Epoch - 1\n",
            "-------------------------------\n",
            "\n",
            "Train Metrics: \n",
            " Accuracy: 74.0%, Avg train loss: 0.839671\n",
            "\n",
            "Val or Test Metrics: \n",
            " Accuracy: 50.0%, Avg loss: 1.241052 \n",
            "\n",
            "Checkpoint guardado en: checkpoint_recovered_epoch_1.pth\n",
            "\n"